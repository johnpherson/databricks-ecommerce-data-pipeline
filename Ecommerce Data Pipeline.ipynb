{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb6129e-c3e5-4e02-8b2e-bdb2e9367b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: pandas>=1.0.0 in /databricks/python3/lib/python3.11/site-packages (from ucimlrepo) (1.5.3)\nRequirement already satisfied: certifi>=2020.12.5 in /databricks/python3/lib/python3.11/site-packages (from ucimlrepo) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2022.7)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (1.23.5)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.0->ucimlrepo) (1.16.0)\nDownloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.7\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2cb66e-c95b-4a7f-8112-5846090363ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFD7️  INITIALIZING DATA PIPELINE\nCatalog: main\nSchema: retail_pipeline\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "CATALOG_NAME = \"main\"\n",
    "SCHEMA_NAME = \"retail_pipeline\"\n",
    "SOURCE_PATH = \"uci_retail_data\"\n",
    "\n",
    "print(\"\uD83C\uDFD7️  INITIALIZING DATA PIPELINE\")\n",
    "print(f\"Catalog: {CATALOG_NAME}\")\n",
    "print(f\"Schema: {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1229f85d-18d3-445a-99d2-1cd32a48ad46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Ingestion Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7381aac8-197e-4b12-b65a-e19d88ba9472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Ingestion...\n\uD83D\uDCE5 BRONZE LAYER: Raw Data Ingestion\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8970861071262534>, line 50\u001B[0m\n",
       "\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# Execute\u001B[39;00m\n",
       "\u001B[1;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Data Ingestion...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 50\u001B[0m bronze_ingestion_count \u001B[38;5;241m=\u001B[39m ingest_raw_data()\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData Ingestion Complete: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze_ingestion_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m records ingested\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-8970861071262534>, line 13\u001B[0m, in \u001B[0;36mingest_raw_data\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m online_retail \u001B[38;5;241m=\u001B[39m fetch_ucirepo(\u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m352\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m raw_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(online_retail\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfeatures)\n",
       "\u001B[0;32m---> 13\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCREATE DATABASE IF NOT EXISTS \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Database \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ready\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# get data from uci\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'CATALOG_NAME' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'CATALOG_NAME' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'CATALOG_NAME' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-8970861071262534>, line 50\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# Execute\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Data Ingestion...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 50\u001B[0m bronze_ingestion_count \u001B[38;5;241m=\u001B[39m ingest_raw_data()\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData Ingestion Complete: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze_ingestion_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m records ingested\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m<command-8970861071262534>, line 13\u001B[0m, in \u001B[0;36mingest_raw_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m online_retail \u001B[38;5;241m=\u001B[39m fetch_ucirepo(\u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m352\u001B[39m)\n\u001B[1;32m     11\u001B[0m raw_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(online_retail\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfeatures)\n\u001B[0;32m---> 13\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCREATE DATABASE IF NOT EXISTS \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Database \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ready\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# get data from uci\u001B[39;00m\n",
        "\u001B[0;31mNameError\u001B[0m: name 'CATALOG_NAME' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ingest_raw_data():\n",
    "    \"\"\"\n",
    "    Ingest raw data from source system\n",
    "    In production: This would connect to APIs, databases, file systems etc\n",
    "    \"\"\"\n",
    "    print(\"\uD83D\uDCE5 BRONZE LAYER: Raw Data Ingestion\")\n",
    "\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    online_retail = fetch_ucirepo(id=352)\n",
    "    raw_df = spark.createDataFrame(online_retail.data.features)\n",
    "\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "    print(f\"✅ Database {CATALOG_NAME}.{SCHEMA_NAME} ready\")\n",
    "\n",
    "    # get data from uci\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    online_retail = fetch_ucirepo(id=352)\n",
    "\n",
    "    raw_df = spark.createDataFrame(online_retail.data.features)\n",
    "    \n",
    "\n",
    "    print(\"Raw data structures\")\n",
    "    display(raw_df)\n",
    "    print(\"Raw data sample\")\n",
    "    display(raw_df.limit(10))\n",
    "\n",
    "\n",
    "    # Add metadata\n",
    "    ingestion_df = raw_df \\\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"source_system\", lit(\"UCI\")) \\\n",
    "        .withColumn(\"batch_id\", lit(\"batch_001\"))\n",
    "                    \n",
    "    # Write data to brone table\n",
    "    ingestion_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(mergeSchema=True) \\\n",
    "        .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.bronze_raw_transactions\")\n",
    "\n",
    "\n",
    "    record_count = ingestion_df.count()\n",
    "    print(f\"✅ {record_count} records ingested to {CATALOG_NAME}.{SCHEMA_NAME}.bronze_raw_transactions\")\n",
    "    return record_count\n",
    "\n",
    "# Execute\n",
    "print(\"Starting Data Ingestion...\")\n",
    "bronze_ingestion_count = ingest_raw_data()\n",
    "print(f\"Data Ingestion Complete: {bronze_ingestion_count} records ingested\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8368e93-1dc4-4ee2-b0d9-da66125ab703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver Layer Data Quality and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f45d35-d84e-471f-9b1b-fa984bff3a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 STARTING SILVER LAYER TRANSFORMATION...\n\uD83E\uDDF9 SILVER LAYER: Data Cleansing and Validation\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8765916674596184>, line 74\u001B[0m\n",
       "\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Execute Silver layer transformation\u001B[39;00m\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\uD83D\uDE80 STARTING SILVER LAYER TRANSFORMATION...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 74\u001B[0m silver_count, quality_rate \u001B[38;5;241m=\u001B[39m create_silver_layer()\n",
       "\u001B[1;32m     75\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\uD83C\uDF89 SILVER LAYER COMPLETE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msilver_count\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m clean records (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquality_rate\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m% pass rate)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-8765916674596184>, line 9\u001B[0m, in \u001B[0;36mcreate_silver_layer\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\uD83E\uDDF9 SILVER LAYER: Data Cleansing and Validation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Pull data in from the Bronze layer\u001B[39;00m\n",
       "\u001B[0;32m----> 9\u001B[0m bronze_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.bronze_raw_transactions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBronze records: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze_df\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Data Quality\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'CATALOG_NAME' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'CATALOG_NAME' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'CATALOG_NAME' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-8765916674596184>, line 74\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Execute Silver layer transformation\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\uD83D\uDE80 STARTING SILVER LAYER TRANSFORMATION...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 74\u001B[0m silver_count, quality_rate \u001B[38;5;241m=\u001B[39m create_silver_layer()\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\uD83C\uDF89 SILVER LAYER COMPLETE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msilver_count\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m clean records (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquality_rate\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m% pass rate)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m<command-8765916674596184>, line 9\u001B[0m, in \u001B[0;36mcreate_silver_layer\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\uD83E\uDDF9 SILVER LAYER: Data Cleansing and Validation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Pull data in from the Bronze layer\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m bronze_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCATALOG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSCHEMA_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.bronze_raw_transactions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBronze records: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbronze_df\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Data Quality\u001B[39;00m\n",
        "\u001B[0;31mNameError\u001B[0m: name 'CATALOG_NAME' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_silver_layer():\n",
    "    \"\"\"\n",
    "    Clean and validate\n",
    "    \"\"\"\n",
    "    print(\"\uD83E\uDDF9 SILVER LAYER: Data Cleansing and Validation\")\n",
    "    \n",
    "    # Pull data in from the Bronze layer\n",
    "\n",
    "    bronze_df = spark.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.bronze_raw_transactions\")\n",
    "\n",
    "    print(f\"Bronze records: {bronze_df.count()}\")\n",
    "    \n",
    "    # Data Quality\n",
    "    print(\"Applying rules\")\n",
    "\n",
    "    silver_df = bronze_df \\\n",
    "        .filter(col(\"InvoiceNo\").cast(\"int\") > 0) \\\n",
    "        .filter(col(\"Quantity\")) \\\n",
    "        .filter(col(\"CustomerID\").isNotNull()) \\\n",
    "        .filter(col(\"Quantity\") > 0) \\\n",
    "        .filter(col(\"UnitPrice\") > 0) \\\n",
    "        .filter(col(\"Description\").isNotNull()) \\\n",
    "        .filter(col(\"InvoiceNo\").isNotNull()) \\\n",
    "        .withColumn(\"InvoiceDateTime\", to_timestamp(col(\"InvoiceDate\"), \"M/d/yyyy H:mm\")) \\\n",
    "        .withColumn(\"Revenue\", col(\"Quantity\") * col(\"UnitPrice\")) \\\n",
    "        .withColumn(\"Year\", year(\"InvoiceDateTime\")) \\\n",
    "        .withColumn(\"Month\", month(\"InvoiceDateTime\")) \\\n",
    "        .withColumn(\"DayOfWeek\", dayofweek(\"InvoiceDateTime\")) \\\n",
    "        .withColumn(\"Quarter\", quarter(\"InvoiceDateTime\")) \\\n",
    "        .withColumn(\"processed_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"data_quality_score\", lit(100))\n",
    "\n",
    "    # Additional business logic\n",
    "    silver_enriched = silver_df \\\n",
    "        .withColumn(\"revenue_category\",\n",
    "                    when(col(\"Revenue\") >= 1000, \"High\") \\\n",
    "                    .when(col(\"Revenue\") >= 500, \"Medium\") \\\n",
    "                    .otherwise(\"Low\")) \\\n",
    "        .withColumn(\"quantity_category\",\n",
    "                    when(col(\"Quantity\") >= 100, \"High\") \\\n",
    "                    .when(col(\"Quantity\") >= 50, \"Medium\") \\\n",
    "                    .otherwise(\"Low\")) \\\n",
    "        .withColumn(\"season\",\n",
    "                    when(col(\"Month\").isin(12, 1, 2), \"Winter\") \n",
    "                    .when(col(\"Month\").isin(3, 4, 5), \"Spring\") \n",
    "                    .when(col(\"Month\").isin(6, 7, 8), \"Summer\") \n",
    "                    .when(col(\"Month\").isin(9, 10, 11), \"Autumn\")) \n",
    "        \n",
    "    # write silver to the delta table\n",
    "    silver_enriched.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(mergeSchema=True) \\\n",
    "        .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.silver_clean_transactions\")\n",
    "\n",
    "    # Data Quality Metrics\n",
    "    bronze_count = bronze_df.count()\n",
    "    silver_count = silver_enriched.count()\n",
    "    quality_pass_rate = (silver_count / bronze_count) * 100\n",
    "\n",
    "    print(f\"✅ Silver layer created: {silver_count:,} records\")\n",
    "    print(f\"\uD83D\uDCCA Data Quality Pass Rate: {quality_pass_rate:.1f}%\")\n",
    "    print(f\"❌ Records filtered out: {bronze_count - silver_count:,}\")\n",
    "    \n",
    "    # Show sample of cleaned data\n",
    "    print(\"\uD83D\uDCCB Sample of Silver data:\")\n",
    "    silver_enriched.select(\"InvoiceNo\", \"Description\", \"Quantity\", \"Revenue\", \n",
    "                          \"revenue_category\", \"season\", \"Year\", \"Month\").show(10)\n",
    "    \n",
    "    return silver_count, quality_pass_rate\n",
    "\n",
    "# Execute Silver layer transformation\n",
    "print(\"\uD83D\uDE80 STARTING SILVER LAYER TRANSFORMATION...\")\n",
    "silver_count, quality_rate = create_silver_layer()\n",
    "print(f\"\uD83C\uDF89 SILVER LAYER COMPLETE: {silver_count:,} clean records ({quality_rate:.1f}% pass rate)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e65c97d-9042-42fa-8703-935a26ccbf18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3b9973a-305e-45ad-a8ff-45f4033659ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def gen_dq_report():\n",
    "    \"\"\"\n",
    "    Generate data quality report\n",
    "    \"\"\"\n",
    "    print(\"\uD83D\uDCCA GENERATING DATA QUALITY REPORT\")  \n",
    "\n",
    "    bronze_df = spark.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.bronze_raw_transactions\")\n",
    "    silver_df = spark.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.silver_clean_transactions\")\n",
    "\n",
    "    bronze_count = bronze_df.count()\n",
    "    silver_count = silver_df.count()\n",
    "    quality_pass_rate = (silver_count / bronze_count) * 100\n",
    "\n",
    "    # Detailed quality checks on Bronze data\n",
    "    quality_checks = {\n",
    "        \"null_customer_ids\": bronze_df.filter(col(\"CustomerID\").isNull()).count(),\n",
    "        \"negative_quantities\": bronze_df.filter(col(\"Quantity\") <= 0).count(),\n",
    "        \"zero_prices\": bronze_df.filter(col(\"UnitPrice\") <= 0).count(),\n",
    "        \"null_descriptions\": bronze_df.filter(col(\"Description\").isNull()).count(),\n",
    "        \"null_invoice_numbers\": bronze_df.filter(col(\"InvoiceNo\").isNull()).count()\n",
    "    }\n",
    "\n",
    "    print(f\"\\n\uD83D\uDCCB DATA QUALITY ISSUES IN BRONZE:\")\n",
    "    total_issues = 0\n",
    "    for issue, count in quality_checks.items():\n",
    "        percentage = (count / bronze_count) * 100\n",
    "        print(f\"  • {issue.replace('_', ' ').title()}: {count:,} ({percentage:.2f}%)\")\n",
    "        total_issues += count\n",
    "\n",
    "        # Business metrics from Silver\n",
    "    print(f\"\\n\uD83D\uDCB0 BUSINESS METRICS FROM SILVER:\")\n",
    "    \n",
    "    revenue_stats = silver_df.agg(\n",
    "        sum(\"Revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"Revenue\").alias(\"avg_revenue\"),\n",
    "        max(\"Revenue\").alias(\"max_revenue\"),\n",
    "        min(\"Revenue\").alias(\"min_revenue\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"  • Total Revenue: £{revenue_stats['total_revenue']:,.2f}\")\n",
    "    print(f\"  • Average Transaction: £{revenue_stats['avg_revenue']:.2f}\")\n",
    "    print(f\"  • Largest Transaction: £{revenue_stats['max_revenue']:.2f}\")\n",
    "    print(f\"  • Smallest Transaction: £{revenue_stats['min_revenue']:.2f}\")\n",
    "    \n",
    "    # Customer and product diversity\n",
    "    unique_customers = silver_df.select(\"CustomerID\").distinct().count()\n",
    "    unique_products = silver_df.select(\"StockCode\").distinct().count()\n",
    "    unique_countries = silver_df.select(\"Country\").distinct().count()\n",
    "    \n",
    "    print(f\"\\n\uD83C\uDF0D DATA DIVERSITY:\")\n",
    "    print(f\"  • Unique Customers: {unique_customers:,}\")\n",
    "    print(f\"  • Unique Products: {unique_products:,}\")\n",
    "    print(f\"  • Countries: {unique_countries:,}\")\n",
    "    \n",
    "    # Revenue categories distribution\n",
    "    print(f\"\\n\uD83D\uDCCA REVENUE DISTRIBUTION:\")\n",
    "    revenue_dist = silver_df.groupBy(\"revenue_category\").count().orderBy(desc(\"count\"))\n",
    "    revenue_dist.show()\n",
    "    \n",
    "    return quality_checks, revenue_stats\n",
    "\n",
    "# Generate the report\n",
    "print(\"\uD83D\uDCCB GENERATING DATA QUALITY REPORT...\")\n",
    "dq_issues, business_metrics = gen_dq_report()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5470740-ec3c-4d86-ba73-89457d80ff2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e189a17f-1f0b-4800-b943-f21f72a3ea05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_gold_layer():\n",
    "    \"\"\"\n",
    "    Create business-ready aggregated tables for analytics and reporting\n",
    "    \"\"\"\n",
    "    print(\"\uD83C\uDFC6 GOLD LAYER: Business Metrics & Aggregations\")\n",
    "    \n",
    "    # Read from Silver layer\n",
    "    silver_df = spark.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.silver_clean_transactions\")\n",
    "    \n",
    "    print(f\"\uD83D\uDCCA Processing {silver_df.count():,} silver records...\")\n",
    "    \n",
    "    print(\"\uD83D\uDCC8 Creating monthly sales metrics...\")\n",
    "    \n",
    "    monthly_metrics = silver_df \\\n",
    "        .groupBy(\"Year\", \"Month\", \"season\") \\\n",
    "        .agg(\n",
    "            sum(\"Revenue\").alias(\"total_revenue\"),\n",
    "            sum(\"Quantity\").alias(\"total_items_sold\"),\n",
    "            count(\"InvoiceNo\").alias(\"total_transactions\"),\n",
    "            countDistinct(\"CustomerID\").alias(\"unique_customers\"),\n",
    "            countDistinct(\"StockCode\").alias(\"unique_products_sold\"),\n",
    "            avg(\"Revenue\").alias(\"avg_transaction_value\"),\n",
    "            max(\"Revenue\").alias(\"max_transaction\"),\n",
    "            min(\"Revenue\").alias(\"min_transaction\")\n",
    "        ) \\\n",
    "        .withColumn(\"revenue_per_customer\", col(\"total_revenue\") / col(\"unique_customers\")) \\\n",
    "        .withColumn(\"items_per_transaction\", col(\"total_items_sold\") / col(\"total_transactions\")) \\\n",
    "        .withColumn(\"created_at\", current_timestamp()) \\\n",
    "        .orderBy(\"Year\", \"Month\")\n",
    "    \n",
    "    # Write monthly metrics\n",
    "    monthly_metrics.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.gold_monthly_metrics\")\n",
    "    \n",
    "    print(\"✅ Monthly metrics table created\")\n",
    "    \n",
    "    print(\"\uD83D\uDC65 Creating customer segmentation...\")\n",
    "    \n",
    "    customer_segments = silver_df \\\n",
    "        .groupBy(\"CustomerID\", \"Country\") \\\n",
    "        .agg(\n",
    "            sum(\"Revenue\").alias(\"total_spent\"),\n",
    "            sum(\"Quantity\").alias(\"total_items_purchased\"),\n",
    "            count(\"InvoiceNo\").alias(\"total_orders\"),\n",
    "            countDistinct(\"StockCode\").alias(\"unique_products_bought\"),\n",
    "            max(\"InvoiceDateTime\").alias(\"last_purchase_date\"),\n",
    "            min(\"InvoiceDateTime\").alias(\"first_purchase_date\"),\n",
    "            avg(\"Revenue\").alias(\"avg_order_value\")\n",
    "        ) \\\n",
    "        .withColumn(\"customer_lifetime_days\", \n",
    "                   datediff(col(\"last_purchase_date\"), col(\"first_purchase_date\"))) \\\n",
    "        .withColumn(\"customer_segment\", \n",
    "                   when(col(\"total_spent\") >= 2000, \"VIP\")\n",
    "                   .when(col(\"total_spent\") >= 1000, \"High Value\")\n",
    "                   .when(col(\"total_spent\") >= 500, \"Medium Value\")\n",
    "                   .otherwise(\"Low Value\")) \\\n",
    "        .withColumn(\"purchase_frequency_segment\",\n",
    "                   when(col(\"total_orders\") >= 50, \"Frequent\")\n",
    "                   .when(col(\"total_orders\") >= 20, \"Regular\")\n",
    "                   .when(col(\"total_orders\") >= 10, \"Occasional\")\n",
    "                   .otherwise(\"Rare\")) \\\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "    \n",
    "    # Write customer segments\n",
    "    customer_segments.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.gold_customer_segments\")\n",
    "    \n",
    "    print(\"✅ Customer segmentation table created\")\n",
    "    \n",
    "    print(\"\uD83D\uDCE6 Creating product performance metrics...\")\n",
    "    \n",
    "    product_metrics = silver_df \\\n",
    "        .groupBy(\"StockCode\", \"Description\") \\\n",
    "        .agg(\n",
    "            sum(\"Revenue\").alias(\"total_revenue\"),\n",
    "            sum(\"Quantity\").alias(\"total_quantity_sold\"),\n",
    "            count(\"InvoiceNo\").alias(\"times_ordered\"),\n",
    "            countDistinct(\"CustomerID\").alias(\"unique_buyers\"),\n",
    "            avg(\"UnitPrice\").alias(\"avg_unit_price\"),\n",
    "            max(\"UnitPrice\").alias(\"max_unit_price\"),\n",
    "            min(\"UnitPrice\").alias(\"min_unit_price\")\n",
    "        ) \\\n",
    "        .withColumn(\"revenue_per_unit\", col(\"total_revenue\") / col(\"total_quantity_sold\")) \\\n",
    "        .withColumn(\"avg_quantity_per_order\", col(\"total_quantity_sold\") / col(\"times_ordered\")) \\\n",
    "        .withColumn(\"product_popularity_score\", \n",
    "                   (col(\"unique_buyers\") * 0.4) + (col(\"times_ordered\") * 0.6)) \\\n",
    "        .withColumn(\"product_performance_tier\",\n",
    "                   when(col(\"total_revenue\") >= 10000, \"Top Performer\")\n",
    "                   .when(col(\"total_revenue\") >= 5000, \"High Performer\")\n",
    "                   .when(col(\"total_revenue\") >= 1000, \"Medium Performer\")\n",
    "                   .otherwise(\"Low Performer\")) \\\n",
    "        .withColumn(\"created_at\", current_timestamp()) \\\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    # Write product metrics\n",
    "    product_metrics.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.gold_product_metrics\")\n",
    "    \n",
    "    print(\"✅ Product performance table created\")\n",
    "    \n",
    "    print(\"\uD83C\uDF0D Creating geographic performance metrics...\")\n",
    "    \n",
    "    country_metrics = silver_df \\\n",
    "        .groupBy(\"Country\") \\\n",
    "        .agg(\n",
    "            sum(\"Revenue\").alias(\"total_revenue\"),\n",
    "            sum(\"Quantity\").alias(\"total_items_sold\"),\n",
    "            count(\"InvoiceNo\").alias(\"total_transactions\"),\n",
    "            countDistinct(\"CustomerID\").alias(\"unique_customers\"),\n",
    "            countDistinct(\"StockCode\").alias(\"unique_products_sold\"),\n",
    "            avg(\"Revenue\").alias(\"avg_transaction_value\")\n",
    "        ) \\\n",
    "        .withColumn(\"revenue_per_customer\", col(\"total_revenue\") / col(\"unique_customers\")) \\\n",
    "        .withColumn(\"market_share_rank\", row_number().over(Window.orderBy(desc(\"total_revenue\")))) \\\n",
    "        .withColumn(\"created_at\", current_timestamp()) \\\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    # Write country metrics\n",
    "    country_metrics.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{CATALOG_NAME}.{SCHEMA_NAME}.gold_country_metrics\")\n",
    "    \n",
    "    print(\"✅ Geographic metrics table created\")\n",
    "    \n",
    "    # Summary of Gold layer\n",
    "    tables_created = [\n",
    "        \"gold_monthly_metrics\",\n",
    "        \"gold_customer_segments\", \n",
    "        \"gold_product_metrics\",\n",
    "        \"gold_country_metrics\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n\uD83C\uDF89 GOLD LAYER COMPLETE!\")\n",
    "    print(\"\uD83D\uDCCA Business-ready tables created:\")\n",
    "    for table in tables_created:\n",
    "        table_count = spark.table(f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table}\").count()\n",
    "        print(f\"  • {table}: {table_count:,} records\")\n",
    "    \n",
    "    return tables_created\n",
    "\n",
    "# Execute Gold layer creation\n",
    "print(\"\uD83D\uDE80 STARTING GOLD LAYER CREATION...\")\n",
    "gold_tables = create_gold_layer()\n",
    "print(f\"\uD83C\uDFC6 GOLD LAYER COMPLETE: {len(gold_tables)} business tables created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ea710a9-1ce7-4ee5-b883-830dce35c057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "BI Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c524f78-9f0a-4be1-9a08-18d207a0ddda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUSINESS INTELLIGENCE VIEWS & INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "def create_business_views():\n",
    "    \"\"\"\n",
    "    Create views for executives, analysts, and dashboard consumption\n",
    "    \"\"\"\n",
    "    print(\"\uD83D\uDCCA CREATING BUSINESS INTELLIGENCE VIEWS\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # EXECUTIVE DASHBOARD VIEW\n",
    "    # =================================================================\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG_NAME}.{SCHEMA_NAME}.executive_dashboard AS\n",
    "    SELECT \n",
    "        Year,\n",
    "        Month,\n",
    "        season,\n",
    "        total_revenue,\n",
    "        unique_customers,\n",
    "        total_transactions,\n",
    "        avg_transaction_value,\n",
    "        revenue_per_customer,\n",
    "        ROUND((total_revenue - LAG(total_revenue) OVER (ORDER BY Year, Month)) / \n",
    "              LAG(total_revenue) OVER (ORDER BY Year, Month) * 100, 2) as revenue_growth_pct,\n",
    "        ROUND((unique_customers - LAG(unique_customers) OVER (ORDER BY Year, Month)) / \n",
    "              LAG(unique_customers) OVER (ORDER BY Year, Month) * 100, 2) as customer_growth_pct\n",
    "    FROM {CATALOG_NAME}.{SCHEMA_NAME}.gold_monthly_metrics\n",
    "    ORDER BY Year, Month\n",
    "    \"\"\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # TOP PERFORMING CUSTOMERS VIEW\n",
    "    # =================================================================\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG_NAME}.{SCHEMA_NAME}.vip_customers AS\n",
    "    SELECT \n",
    "        CustomerID,\n",
    "        Country,\n",
    "        total_spent,\n",
    "        total_orders,\n",
    "        avg_order_value,\n",
    "        customer_segment,\n",
    "        purchase_frequency_segment,\n",
    "        unique_products_bought,\n",
    "        DATEDIFF(CURRENT_DATE(), last_purchase_date) as days_since_last_purchase,\n",
    "        CASE \n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_purchase_date) <= 30 THEN 'Active'\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_purchase_date) <= 90 THEN 'At Risk'\n",
    "            ELSE 'Inactive'\n",
    "        END as customer_status\n",
    "    FROM {CATALOG_NAME}.{SCHEMA_NAME}.gold_customer_segments\n",
    "    WHERE customer_segment IN ('VIP', 'High Value')\n",
    "    ORDER BY total_spent DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # PRODUCT PERFORMANCE DASHBOARD\n",
    "    # =================================================================\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG_NAME}.{SCHEMA_NAME}.top_products AS\n",
    "    SELECT \n",
    "        StockCode,\n",
    "        Description,\n",
    "        total_revenue,\n",
    "        total_quantity_sold,\n",
    "        unique_buyers,\n",
    "        product_performance_tier,\n",
    "        ROUND(product_popularity_score, 2) as popularity_score,\n",
    "        ROUND(revenue_per_unit, 2) as revenue_per_unit,\n",
    "        ROUND(avg_quantity_per_order, 1) as avg_qty_per_order\n",
    "    FROM {CATALOG_NAME}.{SCHEMA_NAME}.gold_product_metrics\n",
    "    WHERE product_performance_tier IN ('Top Performer', 'High Performer')\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"✅ Business Intelligence views created:\")\n",
    "    print(\"  • executive_dashboard\")\n",
    "    print(\"  • vip_customers\") \n",
    "    print(\"  • top_products\")\n",
    "\n",
    "# Create BI views\n",
    "create_business_views()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ecommerce Data Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}